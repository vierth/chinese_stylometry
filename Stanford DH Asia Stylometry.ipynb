{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Humanities Asia Workshop\n",
    "# Stylometerics and Genre Research in Imperial Chinese Studies\n",
    "# Coding for Stylometric Analysis\n",
    "## Paul Vierthaler, Boston College\n",
    "### @pvierth, vierthal@bc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texts encodings\n",
    "It is important to know the encodings of the files you are working with. This is probably one of the largest difficulties faced by people working with Chinese language texts.\n",
    "\n",
    "## Common encodings\n",
    "Chinese language digital texts coming a variety of encodings\n",
    "\n",
    "### UTF-8\n",
    "This is the easiest to work with. UTF, which stands for Unicode Transformation Format, is an international character set that encodes texts in all languages. It extends the ASCII character set and is the most common encoding on the internet.\n",
    "\n",
    "### GB 2312\n",
    "This was the official character set of the People's Republic of China. It is a simplified character set.\n",
    "\n",
    "### GBK\n",
    "GBK extends GB 2312, adding missing characters.\n",
    "\n",
    "### GB 18030\n",
    "GB 18030 was designed to replace GB 2312 and GBK in 2005. It is a Unicode format but maintains compatibility with GB 2312. It also allows for traditional characters, as it is a Unicode encoding.\n",
    "\n",
    "### Big5\n",
    "Big5 is a traditional Chinese format that is common in Taiwan and Hong Kong.\n",
    "\n",
    "## GB 2312 is still very common.\n",
    "Many websites and texts files containing Chinese text still use GB 2312. \n",
    "\n",
    "## We will generally try to convert texts to UTF-8 if they are not already UTF-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Organization\n",
    "If we want to perform Stylometric analysis and compare a variety of texts, it is easiest to ensure that they are all in the same folder. This will allow us to write code that cleans the text and performs the analysis quickly and easily.\n",
    "\n",
    "## Make a Folder for your files.\n",
    "This will need to be in the same folder as this Jupyter notebook. I have provided a collection of files to analyze as part of the workshop. Name the folder something sensible. I have chosen to call the included folder \"corpus.\"\n",
    "\n",
    "## Decide on a way to store metadata.\n",
    "If we want to keep track of information about our texts, we need to decide on a way to do this. I prefer to include a metadata file in the same folder as my Python script that describes each text. Each text is given an idea for a file name. We will use that ID to look up information about the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code!\n",
    "## Opening a file and reading it to string\n",
    "This is how we will be getting our data into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi my nmae is [ai;\n"
     ]
    }
   ],
   "source": [
    "my_file = open(\"test.txt\", \"r\")\n",
    "file_contents = my_file.read()\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Opening parameters\n",
    "open() takes several parameters. First, the file path then the open mode.\n",
    "\"r\" means read\n",
    "\"w\" means write\n",
    "\"a\" means append\n",
    "\n",
    "## Be careful with \"w\"!\n",
    "If you open a file in write mode, if it exists, the program will wipe the files contents without warning you. If it doesn't exist, it will automatically create the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the encoding\n",
    "By default, strings in Python3 are Unicode. If the file you are opening is not unicode, you have to tell python. If it is unicode, you don't have to tell it anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi my nmae is [ai;\n"
     ]
    }
   ],
   "source": [
    "my_file = open(\"test.txt\", \"r\", encoding=\"utf-8\")\n",
    "file_contents = my_file.read()\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Errors\n",
    "When you open many files at once, you will sometimes run in to errors in encoding no matter what you do. You have several options. You can delete the bad character, or replace it with a question mark. The corpus I've provided doesn't have any of these issues, but as you adapt it to run in the wild, you may run in to some issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi my nmae is [ai;\n"
     ]
    }
   ],
   "source": [
    "my_file = open(\"test.txt\", \"r\", encoding=\"utf-8\", errors=\"replace\")\n",
    "file_contents = my_file.read()\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening multiple files.\n",
    "You don't want to open each file you are interested in one at a time. Here we will import a library that will help us with this and save the contents.\n",
    "On Windows machines you will need specify the encoding of a Chinese text file, even when it is UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71012\n",
      "104913\n",
      "63995\n",
      "94443\n",
      "336711\n",
      "150174\n",
      "132795\n",
      "934719\n",
      "881302\n",
      "498485\n",
      "729578\n",
      "884885\n",
      "615823\n",
      "398945\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"corpus\"):\n",
    "    for filename in files:\n",
    "        # I do not want to open hidden files\n",
    "        if filename[0] != \".\":\n",
    "            # open the file\n",
    "            f = open(root + \"/\" + filename, \"r\", encoding = \"utf8\")\n",
    "            # read the contents to a variable\n",
    "            c = f.read()\n",
    "            # make sure to close the file when you are done\n",
    "            f.close()\n",
    "\n",
    "            # check to see if your code is working\n",
    "            # here I am just printing the length of the string\n",
    "            # printing the string would take up a lot of room.\n",
    "            print(len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the information to use later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "info_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"corpus\"):\n",
    "    for filename in files:\n",
    "        if filename[0] != \".\":\n",
    "            f = open(root + \"/\" + filename, \"r\")\n",
    "            c = f.read()\n",
    "            f.close()\n",
    "            info_list.append(c)\n",
    "            \n",
    "for c in info_list:\n",
    "    print(len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cleaning the texts\n",
    "The texts must first be cleaned before they can do anything. The best way to do this is to write a function and will perform the cleaning. We will then call it when we need our texts to be cleaned. We will remove most unwanted characters with regular expressions. As we will have multiple characters not matching the regex, we will use a loop for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(instring):\n",
    "    # Remove mid-file markers\n",
    "    instring = re.sub(r'~~~START\\|.+?\\|START~~~', \"\", instring)\n",
    "    \n",
    "    # This regex will remove all letters and numbers\n",
    "    instring = re.sub(r'[a-zA-Z0-9]', \"\", instring)\n",
    "    \n",
    "    # A variety of characters to remove\n",
    "    unwanted_chars = ['』','。', '！', '，', '：', '、', '（',\n",
    "                      '）', '；', '？', '〉', '〈', '」', '「',\n",
    "                      '『', '“', '”', '!', '\"', '#', '$', '%',\n",
    "                      '&', \"'\", '(', ')', '*', '+', ',', '-',\n",
    "                      '.', '/', \"《\", \"》\", \"·\", \"a\", \"b\"]\n",
    "    \n",
    "    for char in unwanted_chars:\n",
    "        # replace each character with nothing\n",
    "        instring = instring.replace(char, \"\")\n",
    "    \n",
    "    # return the resulting string.\n",
    "    return instring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the text before saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is before:\n",
      "~~~START|0-六十种曲紫钗记-明-汤显祖|STAR\n",
      "This is after: \n",
      "\n",
      "\n",
      "六十种曲 紫钗记 明 汤显祖\n",
      "\n",
      "\n",
      "\n",
      "紫钗记　　明汤显祖\n"
     ]
    }
   ],
   "source": [
    "info_list = []\n",
    "# just for demonstration purposes\n",
    "not_cleaned = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"corpus\"):\n",
    "    for filename in files:\n",
    "        if filename[0] != \".\":\n",
    "            f = open(root + \"/\" + filename, \"r\", encoding=\"utf8\")\n",
    "            c = f.read()\n",
    "            f.close()\n",
    "            \n",
    "            not_cleaned.append(c)\n",
    "            info_list.append(clean(c))\n",
    "\n",
    "print(\"This is before:\" + not_cleaned[0][:30])\n",
    "print(\"This is after: \" + info_list[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Whitespace\n",
    "As there was no whitespace in the original text, you might want to remove it. We can either do this easily with a regular expression in the file-reading stage. We could also add it to the cleaning function if we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is before:\n",
      "~~~START|0-六十种曲紫钗记-明-汤显祖|STAR\n",
      "This is after: 六十种曲紫钗记明汤显祖紫钗记明汤显祖着绣刻演剧十本第二套南西\n"
     ]
    }
   ],
   "source": [
    "info_list = []\n",
    "# just for demonstration purposes\n",
    "not_cleaned = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"corpus\"):\n",
    "    for filename in files:\n",
    "        if filename[0] != \".\":\n",
    "            f = open(root + \"/\" + filename, \"r\", encoding=\"utf8\")\n",
    "            c = f.read()\n",
    "            f.close()\n",
    "            \n",
    "            not_cleaned.append(c)\n",
    "            \n",
    "            # remove white space\n",
    "            c = re.sub(\"\\s+\", \"\", c)\n",
    "            \n",
    "            info_list.append(clean(c))\n",
    "\n",
    "print(\"This is before:\" + not_cleaned[0][:30])\n",
    "print(\"This is after: \" + info_list[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide how to break apart strings\n",
    "Now that we have a clean string to analyze, we will want to decide how to analyze it. The first step is to decide if we want to look at the entire text, or break it apart into equal lengths. There are advantages and disadvantages to each. I will show you how to break apart the texts. To not break the text apart, simply change break_apart to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# This function does not retain the leftover small section at\n",
    "# the end of the text\n",
    "def textBreak(inputstring):\n",
    "    # Decide how long each section should be\n",
    "    divlim = 10000\n",
    "    \n",
    "    # Calculate how many loops to run\n",
    "    loops = len(inputstring)//divlim\n",
    "    \n",
    "    # Make an empty list to save the results\n",
    "    save = []\n",
    "    \n",
    "    # Save chunks of equal length\n",
    "    for i in range(0, loops):\n",
    "        save.append(inputstring[i * divlim: (i + 1) * divlim])\n",
    "    \n",
    "    return save\n",
    "\n",
    "break_apart = True\n",
    "\n",
    "if break_apart == True:\n",
    "    broken_chunks = []\n",
    "\n",
    "    for item in info_list:\n",
    "        broken_chunks.extend(textBreak(item))\n",
    "        \n",
    "# Check to see if it worked.\n",
    "print(len(broken_chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with Metadata\n",
    "If you have structured your data well, you should have a metadata file that keeps track of each document in your corpus. This is not essential, but it is very helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read metadata file into a dictionary.\n",
    "There are a variety of ways of doing this. I am just going to break the text apart and build the dictionary manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'12': ['三国演义', '罗贯中', '明', '小说'], '10': ['西游记', '吴承恩', '明', '小说'], '04': ['儒林外史', '吴敬梓', '清', '小说'], '05': ['十二楼', '李渔', '清', '小说'], '01': ['桃花扇', '孔尚任', '清', '传奇'], '11': ['金瓶梅', '兰陵笑笑生', '明', '小说'], '00': ['紫钗记', '汤显祖', '明', '传奇'], '02': ['清忠谱', '李玉', '清', '传奇'], '13': ['警世通言', '冯梦龙', '明', '话本'], '07': ['水浒传', '施耐庵', '明', '小说'], '09': ['聊斋志异', '蒲松龄', '清', '话本'], '03': ['牡丹亭', '汤显祖', '明', '传奇'], '08': ['红楼梦', '曹雪芹', '清', '小说'], '06': ['无声戏', '李渔', '清', '小说']}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the information\n",
    "metadata = {}\n",
    "\n",
    "# open and extract the string\n",
    "metadatafile = open(\"metadata.txt\", \"r\", encoding=\"utf8\")\n",
    "metadatastring = metadatafile.read()\n",
    "metadatafile.close()\n",
    "\n",
    "# split into by line\n",
    "lines = metadatastring.split(\"\\n\")\n",
    "for line in lines:\n",
    "    # split using tabs\n",
    "    cells = line.split(\"\\t\")\n",
    "    \n",
    "    # use the first column as the key, which I use store\n",
    "    # the rest of the columns\n",
    "    metadata[cells[0]] = cells[1:]\n",
    "    \n",
    "print(metadata)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Metadata to store information about each file\n",
    "I usually store the information in parallel lists. This way it is easy for the analysis part of the software to attach different labels.\n",
    "\n",
    "The following code applies to using whole files (rather than breaking them apart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['紫钗记', '桃花扇', '清忠谱', '牡丹亭', '儒林外史', '十二楼', '无声戏', '水浒传', '红楼梦', '聊斋志异', '西游记', '金瓶梅', '三国演义', '警世通言']\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists to store the information\n",
    "info_list = []\n",
    "title_list = []\n",
    "author_list = []\n",
    "era_list = []\n",
    "genre_list = []\n",
    "\n",
    "# Create dictionaries store unique info:\n",
    "title_author = {}\n",
    "title_era = {}\n",
    "title_genre = {}\n",
    "\n",
    "for root, dirs, files in os.walk(\"corpus\"):\n",
    "    for filename in files:\n",
    "        if filename[0] != \".\":\n",
    "            f = open(root + \"/\" + filename, \"r\", encoding=\"utf8\")\n",
    "            c = f.read()\n",
    "            f.close()\n",
    "            c = re.sub(\"\\s+\", \"\", c)\n",
    "            c = clean(c)\n",
    "            # Get metadata. the [:-4] removes the .txt from filename\n",
    "            metainfo = metadata[filename[:-4]]\n",
    "            \n",
    "            info_list.append(c)\n",
    "            title_list.append(metainfo[0])\n",
    "            author_list.append(metainfo[1])\n",
    "            era_list.append(metainfo[2])\n",
    "            genre_list.append(metainfo[3])\n",
    "            \n",
    "            title_author[metainfo[0]] = metainfo[1]\n",
    "            title_era[metainfo[0]] = metainfo[2]\n",
    "            title_genre[metainfo[0]] = metainfo[3]\n",
    "            \n",
    "print(title_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is a bit more complicated if you break texts apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['汤显祖', '汤显祖', '汤显祖', '汤显祖', '汤显祖', '孔尚任', '孔尚任', '孔尚任', '孔尚任', '孔尚任', '孔尚任', '孔尚任', '孔尚任', '李玉', '李玉', '李玉', '李玉', '李玉', '汤显祖', '汤显祖']\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists/dictionaries to store the information\n",
    "info_list = []\n",
    "title_list = []\n",
    "author_list = []\n",
    "era_list = []\n",
    "genre_list = []\n",
    "title_author = {}\n",
    "title_era = {}\n",
    "title_genre = {}\n",
    "\n",
    "# We should also track which section number\n",
    "section_number = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"corpus\"):\n",
    "    for filename in files:\n",
    "        if filename[0] != \".\":\n",
    "            f = open(root + \"/\" + filename, \"r\", encoding=\"utf8\")\n",
    "            c = f.read()\n",
    "            f.close()\n",
    "            c = re.sub(\"\\s+\", \"\", c)\n",
    "            c = clean(c)\n",
    "            \n",
    "            # Get metadata. the [:-4] removes the .txt from filename\n",
    "            metainfo = metadata[filename[:-4]]\n",
    "            \n",
    "            # The dictionary formation stays the same\n",
    "            title_author[metainfo[0]] = metainfo[1]\n",
    "            title_era[metainfo[0]] = metainfo[2]\n",
    "            title_genre[metainfo[0]] = metainfo[3]\n",
    "        \n",
    "            # Break the Text apart\n",
    "            broken_sections = textBreak(c)\n",
    "            \n",
    "            # We will need to extend, rather than append\n",
    "            info_list.extend(broken_sections)\n",
    "            \n",
    "            title_list.extend([metainfo[0] for i in \n",
    "                               range(0,len(broken_sections))])\n",
    "            author_list.extend([metainfo[1] for i in \n",
    "                                range(0,len(broken_sections))])\n",
    "            era_list.extend([metainfo[2] for i in \n",
    "                             range(0,len(broken_sections))])\n",
    "            genre_list.extend([metainfo[3] for i in \n",
    "                               range(0,len(broken_sections))])\n",
    "            section_number.extend([i for i in range(0, len(broken_sections))])\n",
    "            \n",
    "print(author_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put these two together\n",
    "Let's add some logic so we can easily switch between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty lists/dictionaries to store the information\n",
    "info_list = []\n",
    "title_list = []\n",
    "author_list = []\n",
    "era_list = []\n",
    "genre_list = []\n",
    "section_number = []\n",
    "title_author = {}\n",
    "title_era = {}\n",
    "title_genre = {}\n",
    "\n",
    "break_apart = False\n",
    "\n",
    "for root, dirs, files in os.walk(\"corpus\"):\n",
    "    for filename in files:\n",
    "        if filename[0] != \".\":\n",
    "            f = open(root + \"/\" + filename, \"r\", encoding=\"utf8\")\n",
    "            c = f.read()\n",
    "            f.close()\n",
    "            c = re.sub(\"\\s+\", \"\", c)\n",
    "            c = clean(c)\n",
    "            \n",
    "            # Get metadata. the [:-4] removes the .txt from filename\n",
    "            metainfo = metadata[filename[:-4]]\n",
    "            \n",
    "            title_author[metainfo[0]] = metainfo[1]\n",
    "            title_era[metainfo[0]] = metainfo[2]\n",
    "            title_genre[metainfo[0]] = metainfo[3]\n",
    "            \n",
    "            if not break_apart:\n",
    "                info_list.append(c)\n",
    "                title_list.append(metainfo[0])\n",
    "                author_list.append(metainfo[1])\n",
    "                era_list.append(metainfo[2])\n",
    "                genre_list.append(metainfo[3])\n",
    "                \n",
    "\n",
    "            else:\n",
    "                broken_sections = textBreak(c)\n",
    "                \n",
    "                info_list.extend(broken_sections)\n",
    "\n",
    "                title_list.extend([metainfo[0] for i in \n",
    "                                   range(0,len(broken_sections))])\n",
    "                author_list.extend([metainfo[1] for i in \n",
    "                                    range(0,len(broken_sections))])\n",
    "                era_list.extend([metainfo[2] for i in \n",
    "                                 range(0,len(broken_sections))])\n",
    "                genre_list.extend([metainfo[3] for i in \n",
    "                                   range(0,len(broken_sections))])\n",
    "                section_number.extend([i for i in range(0, len(broken_sections))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can start calculating common characters\n",
    "There are a variety of ways to do this. Here I will just use code packaged in the Sci-kit learn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the text with sci-kit learn vectorizer\n",
    "When you use sci-kit learn, you can give it a lot of options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have a file with whitespace between items:\n",
    "You can use whitespace to tokenize your documents. For example, if you have used the Stanford Word Parser, then you can use a vectorizer set up like this to take advantage of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1),\n",
    "                            token_pattern=\"\\S+\", max_features = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you do not have a file with whitespace between items:\n",
    "Here we will just use characters to tokenize. This works well with Imperial Chinese. Particularly with wenyan texts. Here we are telling it to look at characters, rathern than words, 1-grams, and the 100 most common characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"char\",ngram_range=(1,1),\n",
    "                             max_features = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the vectorizer to the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一', '三', '上', '下', '不', '与', '两', '个', '中', '为', '么', '之', '也', '了', '事', '二', '人', '今', '他', '以', '们', '何', '你', '便', '儿', '先', '公', '军', '出', '到', '前', '十', '去', '又', '只', '叫', '可', '后', '听', '回', '在', '大', '天', '太', '头', '好', '如', '娘', '子', '家', '将', '小', '就', '山', '得', '心', '我', '打', '把', '无', '日', '时', '是', '曰', '有', '来', '正', '此', '玉', '王', '生', '的', '相', '看', '着', '知', '笑', '等', '老', '者', '而', '自', '行', '西', '要', '见', '说', '起', '身', '过', '还', '这', '道', '那', '都', '里', '门', '问', '面', '马']\n"
     ]
    }
   ],
   "source": [
    "word_count_matrix = vectorizer.fit_transform(info_list)\n",
    "# This will tell you the features found by the vectorizer.\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This are not in order of the most common character. We can use a library called pandas to asscertain this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不    76780\n",
      "了    74800\n",
      "一    65474\n",
      "来    55932\n",
      "道    55370\n",
      "人    54672\n",
      "的    47706\n",
      "是    43752\n",
      "我    37794\n",
      "他    34373\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series\n",
    "\n",
    "fullcorpus = \"\"\n",
    "for text in info_list:\n",
    "    fullcorpus += text\n",
    "    \n",
    "tokens = list(fullcorpus)\n",
    "\n",
    "corpus_series = Series(tokens)\n",
    "\n",
    "values = corpus_series.value_counts()\n",
    "\n",
    "print(values[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency and Term Frequency - Inverse Document Frequency\n",
    "sci-kit learn also has a term frequency and tfidf vectorizer that you can use, depending on how you want to think about your texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just use this instead when creating your vectorizer.\n",
    "# To get TF, tell it to not use idf. otherwise, set to true\n",
    "vectorizer = TfidfVectorizer(use_idf=False, analyzer=\"char\",\n",
    "                            ngram_range=(1,1), max_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit more on normalization\n",
    "If you are using texts of different length, you will need to be sure that you use some sort of normalization if you are hoping to use euclidean distance as a similarity measure. One of the easier ways to normalize is to adjust the raw character count to occurrences per thousand characters. The code below does this using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            一          三          上          下          不          与  \\\n",
      "0   27.877143   6.845936  26.273591  11.841618  34.353028   4.687307   \n",
      "1   27.817947  10.084481  20.016744  18.190121  43.001751   3.957683   \n",
      "2   33.331367   6.725267  25.721196  14.453425  39.407705   5.781370   \n",
      "3   29.895996   8.547728  20.337698  13.979536  36.885764   2.863278   \n",
      "4   38.371217   7.975542  15.722156  12.288242  29.767971   4.083773   \n",
      "5   42.140946   4.799286  10.776093   6.458519  63.157895   8.474576   \n",
      "6   41.840849   4.217528  11.221968   6.539955  55.385244   8.546532   \n",
      "7   33.805693   9.313420  19.724803  16.507965  28.553450   7.159230   \n",
      "8   32.400955   3.266042  10.132487   7.366650  39.909375   2.819335   \n",
      "9   32.770973   8.023090   9.766867   8.821963  60.276255  11.699624   \n",
      "10  31.887596  13.490596  14.924652  12.000145  35.605667   8.982989   \n",
      "11  30.403190   6.492741  15.408553  12.897865  34.868650  10.731604   \n",
      "12  25.658167   9.200013  13.090122  17.731512  42.880336  11.944100   \n",
      "13  37.947215   9.809103  13.140194  15.342238  40.445533   9.576887   \n",
      "\n",
      "            两          个          中          为    ...             还  \\\n",
      "0    2.898729  12.951770   9.682990   8.819539    ...      7.586037   \n",
      "1    4.071847  11.188066   8.410077   8.219804    ...      6.659563   \n",
      "2    4.483511  11.857708   6.725267   6.607280    ...      5.958351   \n",
      "3    2.105352  13.979536   9.474083  10.526759    ...      4.926523   \n",
      "4   11.246991  21.725966   5.280104   3.086830    ...      5.154563   \n",
      "5    6.940232  17.341659   8.367529   8.367529    ...      8.688671   \n",
      "6    7.914832  22.536834   7.431767   5.778199    ...      8.695167   \n",
      "7   12.157850  22.373397   9.506045   5.178403    ...      1.836359   \n",
      "8    5.946283  15.019513   4.477768   3.391762    ...      6.660479   \n",
      "9    4.638617   0.188981  17.729826  21.801502    ...      1.417355   \n",
      "10   5.200466  22.828070   6.888301   3.871144    ...      4.890291   \n",
      "11  11.508074  15.502213   7.450489   2.764476    ...      5.519888   \n",
      "12   4.806927   1.642632  17.215802  16.871996    ...      1.738134   \n",
      "13   7.775215  16.431248  13.908907   8.559944    ...      4.259953   \n",
      "\n",
      "            这          道          那         都          里          门  \\\n",
      "0   11.224867   6.352535  11.163192  3.083755   2.096953  11.964969   \n",
      "1   19.636198   6.621508  13.661618  5.175432   7.877312   8.372022   \n",
      "2   13.037579   6.076338  11.090791  3.539614   9.556958   9.792933   \n",
      "3   19.158701   9.937261  13.095288  3.789633  10.021475   8.126658   \n",
      "4   22.508751  33.393888  14.503670  6.469050  18.860679   7.635843   \n",
      "5   16.449599  16.913470   6.155219  7.297056   4.299732   4.281891   \n",
      "6   16.145514  25.472382   7.543244  6.521376   8.249261   4.551957   \n",
      "7   13.641063  34.682137  16.719853  9.936241  17.766449   6.777191   \n",
      "8   20.698307  29.332855  13.032069  7.112535  13.690092   3.327564   \n",
      "9    0.008590   6.167643   0.146031  2.138918   3.461783   7.859879   \n",
      "10  15.250940  44.419469  30.099055  4.749302  11.383823   7.214589   \n",
      "11  14.894935  30.237020  14.795232  6.124145  15.641192  22.136955   \n",
      "12   0.853150   3.482635   1.578964  4.819661   3.196129   4.590456   \n",
      "13  10.129400  30.780564  11.154351  4.780436   9.624932   7.310784   \n",
      "\n",
      "            问         面          马  \n",
      "0    4.687307  3.083755   8.017762  \n",
      "1    7.648984  3.462973   8.257858  \n",
      "2    2.005781  3.362633   2.772698  \n",
      "3    4.589667  3.158028   3.200135  \n",
      "4    4.741016  4.083773   2.747131  \n",
      "5    3.282783  7.154326   0.535236  \n",
      "6    4.143210  5.722461   0.780336  \n",
      "7    3.541090  7.444957  14.170781  \n",
      "8    5.323033  4.755956   0.591151  \n",
      "9   11.751164  1.975707   4.930678  \n",
      "10   3.726128  3.830862   4.568031  \n",
      "11   5.571249  5.121078   2.096772  \n",
      "12   6.519594  5.138000  24.976920  \n",
      "13   5.685276  4.091797   2.490311  \n",
      "\n",
      "[14 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "# Recreate a CountVectorizer object\n",
    "vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(1,1),\n",
    "                             max_features=100)\n",
    "word_count_matrix=vectorizer.fit_transform(info_list)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# We will need a dense matrix, not a sparse matrix\n",
    "dense_words = word_count_matrix.toarray()\n",
    "\n",
    "corpus_dataframe = DataFrame(dense_words, columns=vocab)\n",
    "\n",
    "# Calculate how long each document is\n",
    "doclengths = corpus_dataframe.sum(axis=1)\n",
    "\n",
    "# Make a series that is the same length as the document length series\n",
    "# but populated with 1000.\n",
    "thousand = Series([1000 for i in range(0,len(doclengths))])\n",
    "\n",
    "# Divide this by the length of each document\n",
    "adjusteddoclengths = thousand.divide(doclengths)\n",
    "\n",
    "# Multiply the corpus DataFrame by this adjusting factor\n",
    "per_thousand = corpus_dataframe.multiply(adjusteddoclengths, axis = 0)\n",
    "\n",
    "print(per_thousand)\n",
    "\n",
    "# Convert back to word_count_matrix\n",
    "word_count_matrix = per_thousand.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Vocabulary\n",
    "If you want to, you can give the vectorizer a set vocabulary to pay attention to, rather than just using the most common characters. This comes in handy when you have an idea which characters distinguish the texts most efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_vocab = [\"的\", \"之\", \"曰\", \"说\"]\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"char\",ngram_range=(1,1),\n",
    "                             vocabulary = my_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Cluster Analysis: Making a Dendrogram\n",
    "Now we can start calculating the relationships among these works. We will have to decide if we want to use euclidean distance or cosine similarity. We will import several tools to help us do this\n",
    "\n",
    "### Euclidean Distance\n",
    "Each vector is understood as a point in space. You will need to calculate the distance between each point. We will use these to judge similarity.\n",
    "\n",
    "### Cosine similarity\n",
    "Here we are interested in the direction that each vector points. You will calculate the angle between each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "euc_or_cosine = \"euc\"\n",
    "if euc_or_cosine == \"euc\":\n",
    "    similarity = euclidean_distances(word_count_matrix)\n",
    "elif euc_or_cosine == \"cos\":\n",
    "    similarity = cosine_similarity(word_count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You now have a similarity matrix\n",
    "The similarity variable now contains the similarity measure between each document in the corpus. You can use this to create a linkage matrix which will allow you to visualize the relationships among these texts as a dendrogram.\n",
    "Here we will use the \"Ward\" algorithm to cluster the texts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "linkage_matrix = ward(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is time to visualize the relationships\n",
    "This will open a new window. You will have to close it for later parts of the script to continue to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "# import the plotting library.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "# Set the font to a Chinese Font Family\n",
    "# STHeiti works for Macs, SimHei should work on Windows\n",
    "# Linux does not come with a compatible Chinese font.\n",
    "# Here I have defaulted to a Japanese font.\n",
    "# I've added logic that checks what system you are using.\n",
    "from sys import platform\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    print(\"Sorry, I can't see the appropriate fonts, defaulting to Japanese\")\n",
    "    matplotlib.rc('font', family=\"TakaoPGothic\")\n",
    "elif platform == \"win32\" or platform == \"win64\":\n",
    "    matplotlib.rc('font', family=\"SimHei\")\n",
    "elif platform == \"darwin\":\n",
    "    matplotlib.rc('font', family='STHeiti')\n",
    "    \n",
    "# Make the Dendrogram\n",
    "dendrogram(linkage_matrix, labels=title_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## This can be made a bit more attractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "dendrogram(linkage_matrix, labels=title_list)\n",
    "\n",
    "# Add a Title\n",
    "plt.title(\"Textual Relationships\")\n",
    "\n",
    "# Add x and y axis labels\n",
    "plt.xlabel(\"Texts\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "# Set the angle of the labels so they are easier to read\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Saving the Figure\n",
    "To actually use the results, you will need to save the figure. You can save it as a variety of formats. I advise saving it as a pdf, which you can then edit further in Adobe Illustrator or some other vector-editing program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "# Set the size of the Figure\n",
    "# This will make a Seven inch by Seven inch figure\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "dendrogram(linkage_matrix, labels=title_list)\n",
    "\n",
    "# Add a Title\n",
    "plt.title(\"Textual Relationships\")\n",
    "\n",
    "# Add x and y axis labels\n",
    "plt.xlabel(\"Texts\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "# Set the angle of the labels so they are easier to read\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "plt.savefig(\"results.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding some color\n",
    "Sometimes it helps to add a bit of color to the figure so you can easily interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "dendrogram(linkage_matrix, labels=title_list)\n",
    "plt.title(\"Textual Relationships\")\n",
    "plt.xlabel(\"Texts\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.xticks(rotation=60)\n",
    "\n",
    "# Create a dictionary for color selection\n",
    "# Here we are using genre as the basis for color\n",
    "# You would need to change this if you wanted to color based on authorship.\n",
    "color_dict = {\"传奇\":\"red\", \"小说\":\"blue\", \"话本\":\"magenta\"}\n",
    "\n",
    "# Return information about the tick labels\n",
    "plt_info = plt.gca()\n",
    "tick_labels = plt_info.get_xmajorticklabels()\n",
    "\n",
    "# Iterate through each tick label and assign a new color\n",
    "for tick_label in tick_labels:\n",
    "    # Get the genre from the title to genre dictionary\n",
    "    genre = title_genre[tick_label.get_text()]\n",
    "    \n",
    "    # Get the color from the dictionary\n",
    "    color = color_dict[genre]\n",
    "    \n",
    "    # Set the color\n",
    "    tick_label.set_color(color)\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "There are other ways to visualize the relationships among these texts. Principal component analysis is a way to explore the variance within the dataset. We can use much of the same data that we used for hierarchical cluster analysis.\n",
    "\n",
    "## Sci-kit learn also has the components necessary\n",
    "You will need to import a few new modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components\n",
    "PCA decomposes the dataset into abstracted components that describe the variance. These can be used as axes on which to replot the data. This will often allow you to get the best view of the data (or at least the most comprehensive).\n",
    "\n",
    "### How many components\n",
    "Generally you will only need the first two principal components (which will describe the most variance within the dataset. Sometimes you will be interested in the third and fourth components. For now, just the first two will be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the PCA object\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# PCA requires a dense matrix. word_count_matrix is sparse\n",
    "# unless you ran the normalization to per 1000 code above!\n",
    "# Convert it to dense matrix\n",
    "#dense_words = word_count_matrix.toarray()\n",
    "dense_words = word_count_matrix\n",
    "# Analyze the dataset\n",
    "my_pca = pca.fit(dense_words).transform(dense_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "Ploting the actual PCA is the first task. In a moment we will look at how to plot the loadings.\n",
    "\n",
    "### Setting up to Plot\n",
    "You will need to decide how to visualize the results. Do you want to visualize by author, title, or genre?\n",
    "We will write a function to take care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The input here will be the information you want to use to color\n",
    "# the graph.\n",
    "def info_for_graph(input_list):\n",
    "    # This will return the unique values.\n",
    "    # [a, a, a, b, b] would become\n",
    "    # {a, b}\n",
    "    unique_values = set(input_list)\n",
    "    \n",
    "    # create a list of numerical label and a dictionary to\n",
    "    # populate a list\n",
    "    unique_labels = [i for i in range(0, len(unique_values))]\n",
    "    unique_dictionary = dict(zip(unique_values, unique_labels))\n",
    "    \n",
    "    # make class list\n",
    "    class_list = []\n",
    "    for item in input_list:\n",
    "        class_list.append(unique_dictionary[item])\n",
    "    \n",
    "    return unique_labels, np.array(class_list), unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using this information\n",
    "This function returns everything we will need to properly visualize our Principal component analysis.\n",
    "Call the function and use the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "unique_labels, info_labels, unique_genres = info_for_graph(genre_list)\n",
    "\n",
    "# Make a color list, the same length as unique labels\n",
    "colors = [\"red\", \"magenta\", \"blue\"]\n",
    "\n",
    "# Make the figure\n",
    "plt.figure()\n",
    "\n",
    "# Plot the points using color information.\n",
    "# This code is partially adapted from brandonrose.org/clustering\n",
    "for color, each_class, label in zip(colors, unique_labels, unique_genres):\n",
    "    plt.scatter(my_pca[info_labels == each_class, 0],\n",
    "               my_pca[info_labels == each_class, 1],\n",
    "               label = label, color = color)\n",
    "    \n",
    "# You should title the plot label your axes\n",
    "plt.title(\"Principal Component Analysis\")\n",
    "plt.xlabel(\"PC1: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[0] * 100)+\"%\")\n",
    "plt.ylabel(\"PC2: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[1] * 100)+\"%\")\n",
    "    \n",
    "# Give it a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding labels\n",
    "It is fairly simple to add a line of code that adds labels to the figure. This is useful when you want to know where individual texts fall. It is less useful when you want to plot many texts at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "unique_labels, info_labels, unique_genres = info_for_graph(genre_list)\n",
    "colors = [\"red\", \"magenta\", \"blue\"]\n",
    "plt.figure()\n",
    "\n",
    "for color, each_class, label in zip(colors, unique_labels, unique_genres):\n",
    "    plt.scatter(my_pca[info_labels == each_class, 0],\n",
    "               my_pca[info_labels == each_class, 1],\n",
    "               label = label, color = color)\n",
    "    \n",
    "for i, text_label in enumerate(title_list):\n",
    "    plt.annotate(text_label,  xy = (my_pca[i, 0], my_pca[i, 1]),\n",
    "                 xytext=(my_pca[i, 0], my_pca[i, 1]), \n",
    "                 size=8)\n",
    "plt.title(\"Principal Component Analysis\")\n",
    "plt.xlabel(\"PC1: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[0] * 100)+\"%\")\n",
    "plt.ylabel(\"PC2: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[1] * 100)+\"%\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loadings Plot\n",
    "You will often want to know how the individual variables have influenced where each text falls. The following code will create a loadings plot using the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "loadings = pca.components_\n",
    "\n",
    "# This will plot the locations of the loadings, but make the \n",
    "# points completely transparent.\n",
    "plt.scatter(loadings[0], loadings[1], alpha=0)\n",
    "\n",
    "# Label and Title\n",
    "plt.title(\"Principal Component Loadings\")\n",
    "plt.xlabel(\"PC1: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[0] * 100)+\"%\")\n",
    "plt.ylabel(\"PC2: \" + \"{0:.2f}\".format(pca.explained_variance_ratio_[1] * 100)+\"%\")\n",
    "\n",
    "# Iterate through the vocab and plot where it falls on loadings graph\n",
    "# numpy array the loadings info is held in is in the opposite format of the\n",
    "# pca information\n",
    "for i, txt in enumerate(vocab):\n",
    "    plt.annotate(txt, (loadings[0, i], loadings[1, i]), horizontalalignment='center',\n",
    "                 verticalalignment='center', size=8)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
